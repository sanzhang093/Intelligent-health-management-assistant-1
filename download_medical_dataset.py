#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ä¸‹è½½åŒ»å­¦æ¨ç†æ•°æ®é›†
ä»Hugging Faceä¸‹è½½FreedomIntelligence/medical-o1-reasoning-SFTæ•°æ®é›†
"""

import os
import json
from datasets import load_dataset
from huggingface_hub import hf_hub_download

def download_medical_dataset(config_name="zh"):
    """ä¸‹è½½åŒ»å­¦æ¨ç†æ•°æ®é›†"""
    print("ğŸ¥ å¼€å§‹ä¸‹è½½åŒ»å­¦æ¨ç†æ•°æ®é›†...")
    print("=" * 60)
    
    # åˆ›å»ºæ•°æ®ç›®å½•
    data_dir = "data"
    medical_data_dir = os.path.join(data_dir, "medical_dataset")
    
    if not os.path.exists(medical_data_dir):
        os.makedirs(medical_data_dir)
        print(f"ğŸ“ åˆ›å»ºæ•°æ®ç›®å½•: {medical_data_dir}")
    
    try:
        # æ•°æ®é›†åç§°
        dataset_name = "FreedomIntelligence/medical-o1-reasoning-SFT"
        
        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½æ•°æ®é›†: {dataset_name}")
        print(f"ğŸ“‹ é…ç½®: {config_name}")
        print("è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        # ä¸‹è½½æ•°æ®é›†
        dataset = load_dataset(dataset_name, config_name)
        
        print("âœ… æ•°æ®é›†ä¸‹è½½æˆåŠŸï¼")
        print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
        print(f"   - æ•°æ®é›†åç§°: {dataset_name}")
        print(f"   - åˆ†å‰²æ•°é‡: {len(dataset)}")
        
        # ä¿å­˜å„ä¸ªåˆ†å‰²çš„æ•°æ®
        for split_name, split_data in dataset.items():
            print(f"\nğŸ“‹ å¤„ç†åˆ†å‰²: {split_name}")
            print(f"   - æ ·æœ¬æ•°é‡: {len(split_data)}")
            
            # ä¿å­˜ä¸ºJSONæ–‡ä»¶
            output_file = os.path.join(medical_data_dir, f"{split_name}.json")
            
            # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶ä¿å­˜
            data_list = []
            for i, item in enumerate(split_data):
                data_list.append(item)
                if (i + 1) % 1000 == 0:
                    print(f"   - å·²å¤„ç† {i + 1}/{len(split_data)} ä¸ªæ ·æœ¬")
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(data_list, f, ensure_ascii=False, indent=2)
            
            print(f"   âœ… å·²ä¿å­˜åˆ°: {output_file}")
            print(f"   ğŸ“Š æ–‡ä»¶å¤§å°: {os.path.getsize(output_file) / 1024 / 1024:.2f} MB")
        
        # åˆ›å»ºæ•°æ®é›†ä¿¡æ¯æ–‡ä»¶
        info_file = os.path.join(medical_data_dir, "dataset_info.json")
        dataset_info = {
            "dataset_name": dataset_name,
            "description": "åŒ»å­¦æ¨ç†æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒåŒ»å­¦LLMçš„å¤æ‚æ¨ç†èƒ½åŠ›",
            "splits": list(dataset.keys()),
            "total_samples": sum(len(split) for split in dataset.values()),
            "download_date": str(pd.Timestamp.now()),
            "source": "https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT"
        }
        
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(dataset_info, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ æ•°æ®é›†ä¿¡æ¯å·²ä¿å­˜åˆ°: {info_file}")
        
        # æ˜¾ç¤ºæ ·æœ¬é¢„è§ˆ
        print(f"\nğŸ“– æ•°æ®é›†æ ·æœ¬é¢„è§ˆ:")
        print("=" * 60)
        
        # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬
        first_split = list(dataset.keys())[0]
        first_sample = dataset[first_split][0]
        
        print(f"åˆ†å‰²: {first_split}")
        print(f"æ ·æœ¬å­—æ®µ: {list(first_sample.keys())}")
        print(f"\né—®é¢˜ç¤ºä¾‹:")
        print(f"é—®é¢˜: {first_sample.get('Question', 'N/A')[:200]}...")
        print(f"\næ¨ç†é“¾ç¤ºä¾‹:")
        print(f"æ¨ç†: {first_sample.get('Complex_CoT', 'N/A')[:300]}...")
        print(f"\nå›ç­”ç¤ºä¾‹:")
        print(f"å›ç­”: {first_sample.get('Response', 'N/A')[:200]}...")
        
        print(f"\nğŸ‰ æ•°æ®é›†ä¸‹è½½å®Œæˆï¼")
        print(f"ğŸ“ æ•°æ®ä¿å­˜åœ¨: {medical_data_dir}")
        
        return medical_data_dir
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def analyze_dataset():
    """åˆ†æä¸‹è½½çš„æ•°æ®é›†"""
    print("\nğŸ” åˆ†ææ•°æ®é›†...")
    print("=" * 60)
    
    medical_data_dir = os.path.join("data", "medical_dataset")
    
    if not os.path.exists(medical_data_dir):
        print("âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨ï¼Œè¯·å…ˆä¸‹è½½æ•°æ®é›†")
        return
    
    # è¯»å–æ•°æ®é›†ä¿¡æ¯
    info_file = os.path.join(medical_data_dir, "dataset_info.json")
    if os.path.exists(info_file):
        with open(info_file, 'r', encoding='utf-8') as f:
            info = json.load(f)
        
        print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        print(f"   - æ•°æ®é›†åç§°: {info['dataset_name']}")
        print(f"   - æ€»æ ·æœ¬æ•°: {info['total_samples']}")
        print(f"   - åˆ†å‰²: {', '.join(info['splits'])}")
        print(f"   - ä¸‹è½½æ—¶é—´: {info['download_date']}")
    
    # åˆ†æå„ä¸ªåˆ†å‰²
    for filename in os.listdir(medical_data_dir):
        if filename.endswith('.json') and filename != 'dataset_info.json':
            split_name = filename.replace('.json', '')
            filepath = os.path.join(medical_data_dir, filename)
            
            print(f"\nğŸ“‹ åˆ†å‰²: {split_name}")
            
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                print(f"   - æ ·æœ¬æ•°é‡: {len(data)}")
                print(f"   - æ–‡ä»¶å¤§å°: {os.path.getsize(filepath) / 1024 / 1024:.2f} MB")
                
                if data:
                    sample = data[0]
                    print(f"   - å­—æ®µ: {list(sample.keys())}")
                    
                    # ç»Ÿè®¡å­—æ®µé•¿åº¦
                    for field in ['Question', 'Complex_CoT', 'Response']:
                        if field in sample:
                            avg_length = sum(len(str(item.get(field, ''))) for item in data[:100]) / min(100, len(data))
                            print(f"   - {field} å¹³å‡é•¿åº¦: {avg_length:.0f} å­—ç¬¦")
                
            except Exception as e:
                print(f"   âŒ åˆ†æå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¥ åŒ»å­¦æ¨ç†æ•°æ®é›†ä¸‹è½½å·¥å…·")
    print("=" * 60)
    print("æ•°æ®é›†: FreedomIntelligence/medical-o1-reasoning-SFT")
    print("ç”¨é€”: è®­ç»ƒåŒ»å­¦LLMçš„å¤æ‚æ¨ç†èƒ½åŠ›")
    print("=" * 60)
    
    # æ˜¾ç¤ºå¯ç”¨çš„é…ç½®é€‰é¡¹
    available_configs = ['en', 'zh', 'en_mix', 'zh_mix']
    print(f"\nğŸ“‹ å¯ç”¨çš„æ•°æ®é›†é…ç½®:")
    for i, config in enumerate(available_configs, 1):
        print(f"   {i}. {config}")
    
    # é€‰æ‹©é…ç½®
    while True:
        try:
            choice = input(f"\nè¯·é€‰æ‹©é…ç½® (1-{len(available_configs)}) æˆ–ç›´æ¥è¾“å…¥é…ç½®å: ").strip()
            
            if choice.isdigit():
                config_index = int(choice) - 1
                if 0 <= config_index < len(available_configs):
                    config_name = available_configs[config_index]
                    break
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
            elif choice in available_configs:
                config_name = choice
                break
            else:
                print("âŒ æ— æ•ˆé…ç½®åï¼Œè¯·é‡æ–°è¾“å…¥")
        except ValueError:
            print("âŒ æ— æ•ˆè¾“å…¥ï¼Œè¯·é‡æ–°è¾“å…¥")
    
    print(f"âœ… å·²é€‰æ‹©é…ç½®: {config_name}")
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æ•°æ®é›†
    medical_data_dir = os.path.join("data", "medical_dataset")
    if os.path.exists(medical_data_dir) and os.listdir(medical_data_dir):
        print("ğŸ“ æ£€æµ‹åˆ°å·²å­˜åœ¨çš„æ•°æ®é›†")
        choice = input("æ˜¯å¦é‡æ–°ä¸‹è½½ï¼Ÿ(y/n): ").strip().lower()
        if choice != 'y':
            print("ğŸ“Š åˆ†æç°æœ‰æ•°æ®é›†...")
            analyze_dataset()
            return
    
    # ä¸‹è½½æ•°æ®é›†
    result = download_medical_dataset(config_name)
    
    if result:
        # åˆ†ææ•°æ®é›†
        analyze_dataset()
        
        print(f"\nâœ… æ•°æ®é›†ä¸‹è½½å’Œåˆ†æå®Œæˆï¼")
        print(f"ğŸ“ æ•°æ®ä½ç½®: {result}")
        print(f"ğŸ’¡ æ‚¨å¯ä»¥åœ¨å¥åº·ç®¡ç†ç³»ç»Ÿä¸­ä½¿ç”¨è¿™äº›åŒ»å­¦æ¨ç†æ•°æ®æ¥å¢å¼ºAIçš„åŒ»å­¦çŸ¥è¯†")
    else:
        print("âŒ æ•°æ®é›†ä¸‹è½½å¤±è´¥")

if __name__ == '__main__':
    import pandas as pd
    main()
