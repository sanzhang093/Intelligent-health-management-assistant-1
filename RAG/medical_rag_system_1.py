#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
åŒ»ç–—RAGç³»ç»Ÿ - ç‰ˆæœ¬1
åŸºäºQwen-Maxå’ŒåŒ»å­¦æ¨ç†æ•°æ®é›†çš„æ™ºèƒ½åŒ»ç–—é—®ç­”ç³»ç»Ÿ
ä¿®å¤äº†è·¯å¾„é—®é¢˜ï¼Œç¡®ä¿èƒ½æ­£ç¡®æ‰¾åˆ°åŒ»å­¦æ•°æ®æ–‡ä»¶
"""

import os
import json
import logging
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import pickle
import hashlib

# å‘é‡åŒ–å’Œæ£€ç´¢ç›¸å…³
import faiss
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

# Qwenç›¸å…³
import dashscope
from qwen_agent.agents import Assistant

logger = logging.getLogger(__name__)

class MedicalRAGSystem:
    """åŒ»ç–—RAGç³»ç»Ÿ"""
    
    def __init__(self, 
                 embedding_model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
                 chunk_size: int = 1000,
                 chunk_overlap: int = 200,
                 vector_db_path: str = "vector_db",
                 medical_data_path: str = "../data/medical_dataset"):
        """
        åˆå§‹åŒ–åŒ»ç–—RAGç³»ç»Ÿ
        
        Args:
            embedding_model_name: åµŒå…¥æ¨¡å‹åç§°
            chunk_size: æ–‡æœ¬åˆ†å—å¤§å°
            chunk_overlap: åˆ†å—é‡å å¤§å°
            vector_db_path: å‘é‡æ•°æ®åº“è·¯å¾„
            medical_data_path: åŒ»å­¦æ•°æ®è·¯å¾„
        """
        self.embedding_model_name = embedding_model_name
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.vector_db_path = vector_db_path
        self.medical_data_path = medical_data_path
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.embedding_model = None
        self.text_splitter = None
        self.vector_index = None
        self.documents = []
        self.metadata = []
        
        # Qwené…ç½®
        self.llm_config = {
            'model': 'qwen-max',
            'timeout': 60,
            'retry_count': 3,
        }
        self.assistant = None
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        self._create_directories()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_components()
    
    def _create_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        os.makedirs(self.vector_db_path, exist_ok=True)
        os.makedirs("cache", exist_ok=True)
        os.makedirs("logs", exist_ok=True)
    
    def _init_components(self):
        """åˆå§‹åŒ–å„ä¸ªç»„ä»¶"""
        try:
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            print("ğŸ”„ æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹...")
            self.embedding_model = SentenceTransformer(self.embedding_model_name)
            print(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: {self.embedding_model_name}")
            
            # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
                length_function=len,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
            )
            
            # åˆå§‹åŒ–QwenåŠ©æ‰‹
            self._init_qwen_assistant()
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰å‘é‡æ•°æ®åº“
            if self._vector_db_exists():
                print("ğŸ“ å‘ç°å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“ï¼Œæ­£åœ¨åŠ è½½...")
                self._load_vector_db()
            else:
                print("ğŸ”„ æ„å»ºæ–°çš„å‘é‡æ•°æ®åº“...")
                self._build_vector_db()
                
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–ç»„ä»¶å¤±è´¥: {e}")
            raise
    
    def _init_qwen_assistant(self):
        """åˆå§‹åŒ–QwenåŠ©æ‰‹"""
        try:
            # é…ç½®DashScope API Key
            dashscope.api_key = os.getenv('DASHSCOPE_API_KEY', 'your_dashscope_api_key_here')
            dashscope.timeout = 60
            
            system_prompt = self._get_medical_system_prompt()
            
            self.assistant = Assistant(
                llm=self.llm_config,
                name='åŒ»ç–—RAGåŠ©æ‰‹',
                description='åŸºäºåŒ»å­¦çŸ¥è¯†åº“çš„ä¸“ä¸šåŒ»ç–—é—®ç­”åŠ©æ‰‹',
                system_message=system_prompt
            )
            
            print("âœ… QwenåŠ©æ‰‹åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"QwenåŠ©æ‰‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _get_medical_system_prompt(self) -> str:
        """è·å–åŒ»ç–—ç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ»ç–—AIåŠ©æ‰‹ï¼Œå…·æœ‰ä¸°å¯Œçš„åŒ»å­¦çŸ¥è¯†å’Œä¸´åºŠç»éªŒã€‚

ä½ çš„èŒè´£æ˜¯ï¼š
1. ç—‡çŠ¶æŸ¥è¯¢ï¼šå¸®åŠ©ç”¨æˆ·ç†è§£ç—‡çŠ¶çš„å¯èƒ½åŸå› å’Œåº”å¯¹æ–¹æ³•
2. è¯ç‰©ä¿¡æ¯ï¼šæä¾›å‡†ç¡®çš„è¯ç‰©ä½¿ç”¨æŒ‡å¯¼å’Œå®‰å…¨ä¿¡æ¯
3. å¥åº·æŒ‡æ ‡è§£è¯»ï¼šè§£é‡Šå„ç§å¥åº·æ£€æŸ¥æŒ‡æ ‡çš„å«ä¹‰
4. ç´§æ€¥æƒ…å†µå¤„ç†ï¼šè¯†åˆ«ç´§æ€¥åŒ»ç–—æƒ…å†µå¹¶æä¾›åº”æ€¥æŒ‡å¯¼

å›ç­”åŸåˆ™ï¼š
- åŸºäºæä¾›çš„åŒ»å­¦çŸ¥è¯†åº“è¿›è¡Œå›ç­”
- æä¾›å‡†ç¡®ã€ç§‘å­¦çš„åŒ»ç–—ä¿¡æ¯
- å¯¹äºç´§æ€¥æƒ…å†µï¼Œæ˜ç¡®å»ºè®®ç«‹å³å°±åŒ»
- é¿å…ç»™å‡ºå…·ä½“çš„è¯Šæ–­å»ºè®®ï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿ
- å¼•ç”¨ç›¸å…³çš„åŒ»å­¦æ–‡çŒ®å’ŒçŸ¥è¯†æ¥æº

å›ç­”æ ¼å¼ï¼š
1. ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜
2. æä¾›è¯¦ç»†çš„è§£é‡Šå’ŒèƒŒæ™¯ä¿¡æ¯
3. ç»™å‡ºå®ç”¨çš„å»ºè®®å’Œæ³¨æ„äº‹é¡¹
4. åœ¨å¿…è¦æ—¶å»ºè®®å°±åŒ»
5. åˆ—å‡ºå‚è€ƒæ¥æº

è¯·å§‹ç»ˆä»¥ç”¨æˆ·çš„å®‰å…¨å’Œå¥åº·ä¸ºé¦–è¦è€ƒè™‘ã€‚"""
    
    def _vector_db_exists(self) -> bool:
        """æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦å­˜åœ¨"""
        index_file = os.path.join(self.vector_db_path, "faiss_index.bin")
        metadata_file = os.path.join(self.vector_db_path, "metadata.pkl")
        return os.path.exists(index_file) and os.path.exists(metadata_file)
    
    def _build_vector_db(self):
        """æ„å»ºå‘é‡æ•°æ®åº“"""
        try:
            print("ğŸ“š æ­£åœ¨åŠ è½½åŒ»å­¦æ•°æ®é›†...")
            
            # åŠ è½½åŒ»å­¦æ•°æ®
            medical_data = self._load_medical_data()
            
            if not medical_data:
                raise ValueError("æœªæ‰¾åˆ°åŒ»å­¦æ•°æ®")
            
            print(f"ğŸ“Š åŠ è½½äº† {len(medical_data)} æ¡åŒ»å­¦æ•°æ®")
            
            # å¤„ç†æ–‡æ¡£
            print("ğŸ”„ æ­£åœ¨å¤„ç†æ–‡æ¡£...")
            documents = self._process_documents(medical_data)
            
            print(f"ğŸ“„ ç”Ÿæˆäº† {len(documents)} ä¸ªæ–‡æ¡£å—")
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            print("ğŸ”„ æ­£åœ¨ç”ŸæˆåµŒå…¥å‘é‡...")
            embeddings = self._generate_embeddings(documents)
            
            # æ„å»ºFAISSç´¢å¼•
            print("ğŸ”„ æ­£åœ¨æ„å»ºå‘é‡ç´¢å¼•...")
            self._build_faiss_index(embeddings)
            
            # ä¿å­˜å…ƒæ•°æ®
            self.documents = documents
            self._save_metadata()
            
            print("âœ… å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ„å»ºå‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            raise
    
    def _load_medical_data(self) -> List[Dict]:
        """åŠ è½½åŒ»å­¦æ•°æ®"""
        try:
            # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
            script_dir = os.path.dirname(os.path.abspath(__file__))
            current_dir = os.getcwd()
            
            print(f"ğŸ” è„šæœ¬ç›®å½•: {script_dir}")
            print(f"ğŸ“ å½“å‰å·¥ä½œç›®å½•: {current_dir}")
            
            # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
            possible_paths = [
                # ç›¸å¯¹äºè„šæœ¬ç›®å½•çš„è·¯å¾„
                os.path.join(script_dir, "../data/medical_dataset/train.json"),
                os.path.join(script_dir, "data/medical_dataset/train.json"),
                # ç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•çš„è·¯å¾„
                os.path.join(current_dir, "12-Agentæ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ä¸åº”ç”¨/CASE-æ™ºèƒ½å¥åº·ç®¡ç†åŠ©æ‰‹/data/medical_dataset/train.json"),
                os.path.join(current_dir, "data/medical_dataset/train.json"),
                # å…¶ä»–å¯èƒ½çš„è·¯å¾„
                "data/medical_dataset/train.json",
                "../data/medical_dataset/train.json",
                "../../data/medical_dataset/train.json"
            ]
            
            data_file = None
            for path in possible_paths:
                abs_path = os.path.abspath(path)
                print(f"ğŸ” å°è¯•è·¯å¾„: {abs_path}")
                if os.path.exists(abs_path):
                    data_file = abs_path
                    print(f"âœ… æ‰¾åˆ°åŒ»å­¦æ•°æ®æ–‡ä»¶: {data_file}")
                    break
            
            if not data_file:
                raise FileNotFoundError(f"åŒ»å­¦æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå·²å°è¯•çš„è·¯å¾„: {possible_paths}")
            
            with open(data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"âœ… æˆåŠŸåŠ è½½åŒ»å­¦æ•°æ®ï¼Œå…± {len(data)} æ¡è®°å½•")
            return data
            
        except Exception as e:
            logger.error(f"åŠ è½½åŒ»å­¦æ•°æ®å¤±è´¥: {e}")
            return []
    
    def _process_documents(self, medical_data: List[Dict]) -> List[Document]:
        """å¤„ç†æ–‡æ¡£ï¼Œç”Ÿæˆæ–‡æ¡£å—"""
        documents = []
        total_items = len(medical_data)
        
        print(f"ğŸ“„ å¼€å§‹å¤„ç† {total_items} æ¡åŒ»å­¦æ•°æ®...")
        
        for i, item in enumerate(medical_data):
            # æ˜¾ç¤ºè¿›åº¦
            if (i + 1) % 1000 == 0 or i == 0:
                print(f"ğŸ”„ å¤„ç†è¿›åº¦: {i+1}/{total_items} ({((i+1)/total_items)*100:.1f}%)")
            
            # åˆ›å»ºæ–‡æ¡£å†…å®¹
            content = f"""
é—®é¢˜: {item.get('Question', '')}

æ¨ç†è¿‡ç¨‹: {item.get('Complex_CoT', '')}

ç­”æ¡ˆ: {item.get('Response', '')}
"""
            
            # åˆ›å»ºå…ƒæ•°æ®
            metadata = {
                'source': 'medical_dataset',
                'index': i,
                'question': item.get('Question', '')[:100] + '...' if len(item.get('Question', '')) > 100 else item.get('Question', ''),
                'original_data': item
            }
            
            # åˆ›å»ºDocumentå¯¹è±¡
            doc = Document(page_content=content, metadata=metadata)
            
            # åˆ†å‰²æ–‡æ¡£
            chunks = self.text_splitter.split_documents([doc])
            
            # ä¸ºæ¯ä¸ªå—æ·»åŠ å”¯ä¸€ID
            for j, chunk in enumerate(chunks):
                chunk.metadata['chunk_id'] = f"{i}_{j}"
                chunk.metadata['chunk_index'] = j
                documents.append(chunk)
        
        print(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(documents)} ä¸ªæ–‡æ¡£å—")
        return documents
    
    def _generate_embeddings(self, documents: List[Document]) -> np.ndarray:
        """ç”Ÿæˆæ–‡æ¡£åµŒå…¥å‘é‡"""
        texts = [doc.page_content for doc in documents]
        total_texts = len(texts)
        
        print(f"ğŸ”„ å¼€å§‹ç”ŸæˆåµŒå…¥å‘é‡ï¼Œå…± {total_texts} ä¸ªæ–‡æ¡£...")
        print("â° é¢„è®¡éœ€è¦ 1-2 å°æ—¶ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        # æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡ï¼Œä½¿ç”¨æ‰¹å¤„ç†æé«˜æ•ˆç‡
        embeddings = self.embedding_model.encode(
            texts, 
            show_progress_bar=True,
            batch_size=32,  # æ§åˆ¶æ‰¹å¤„ç†å¤§å°
            convert_to_numpy=True
        )
        
        print(f"âœ… åµŒå…¥å‘é‡ç”Ÿæˆå®Œæˆï¼Œç»´åº¦: {embeddings.shape}")
        return embeddings.astype('float32')
    
    def _build_faiss_index(self, embeddings: np.ndarray):
        """æ„å»ºFAISSç´¢å¼•"""
        dimension = embeddings.shape[1]
        
        print(f"ğŸ”„ å¼€å§‹æ„å»ºFAISSç´¢å¼•ï¼Œç»´åº¦: {dimension}, å‘é‡æ•°: {embeddings.shape[0]}")
        
        # åˆ›å»ºFAISSç´¢å¼•
        self.vector_index = faiss.IndexFlatIP(dimension)  # ä½¿ç”¨å†…ç§¯ç›¸ä¼¼åº¦
        
        # å½’ä¸€åŒ–å‘é‡ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        print("ğŸ”„ æ­£åœ¨å½’ä¸€åŒ–å‘é‡...")
        faiss.normalize_L2(embeddings)
        
        # æ·»åŠ å‘é‡åˆ°ç´¢å¼•
        print("ğŸ”„ æ­£åœ¨æ·»åŠ å‘é‡åˆ°ç´¢å¼•...")
        self.vector_index.add(embeddings)
        
        # ä¿å­˜ç´¢å¼•åˆ°å½“å‰RAGæ–‡ä»¶å¤¹
        index_file = os.path.join(self.vector_db_path, "faiss_index.bin")
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ç´¢å¼•æ–‡ä»¶: {os.path.abspath(index_file)}")
        faiss.write_index(self.vector_index, index_file)
        
        # éªŒè¯æ–‡ä»¶æ˜¯å¦ä¿å­˜æˆåŠŸ
        if os.path.exists(index_file):
            file_size = os.path.getsize(index_file) / (1024 * 1024)  # MB
            print(f"âœ… FAISSç´¢å¼•æ„å»ºå®Œæˆï¼Œç»´åº¦: {dimension}, å‘é‡æ•°: {embeddings.shape[0]}")
            print(f"ğŸ“ ç´¢å¼•æ–‡ä»¶å·²ä¿å­˜: {os.path.abspath(index_file)} ({file_size:.2f} MB)")
        else:
            raise RuntimeError(f"ç´¢å¼•æ–‡ä»¶ä¿å­˜å¤±è´¥: {index_file}")
    
    def _save_metadata(self):
        """ä¿å­˜å…ƒæ•°æ®"""
        metadata_file = os.path.join(self.vector_db_path, "metadata.pkl")
        
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜å…ƒæ•°æ®æ–‡ä»¶: {os.path.abspath(metadata_file)}")
        
        metadata = {
            'documents': self.documents,
            'embedding_model': self.embedding_model_name,
            'chunk_size': self.chunk_size,
            'chunk_overlap': self.chunk_overlap,
            'created_at': datetime.now().isoformat(),
            'total_chunks': len(self.documents),
            'vector_db_path': os.path.abspath(self.vector_db_path)
        }
        
        with open(metadata_file, 'wb') as f:
            pickle.dump(metadata, f)
        
        # éªŒè¯æ–‡ä»¶æ˜¯å¦ä¿å­˜æˆåŠŸ
        if os.path.exists(metadata_file):
            file_size = os.path.getsize(metadata_file) / (1024 * 1024)  # MB
            print(f"âœ… å…ƒæ•°æ®å·²ä¿å­˜: {os.path.abspath(metadata_file)} ({file_size:.2f} MB)")
            print(f"ğŸ“Š å…ƒæ•°æ®ä¿¡æ¯: {len(self.documents)} ä¸ªæ–‡æ¡£å—")
        else:
            raise RuntimeError(f"å…ƒæ•°æ®æ–‡ä»¶ä¿å­˜å¤±è´¥: {metadata_file}")
    
    def _load_vector_db(self):
        """åŠ è½½å‘é‡æ•°æ®åº“"""
        try:
            # åŠ è½½FAISSç´¢å¼•
            index_file = os.path.join(self.vector_db_path, "faiss_index.bin")
            self.vector_index = faiss.read_index(index_file)
            
            # åŠ è½½å…ƒæ•°æ®
            metadata_file = os.path.join(self.vector_db_path, "metadata.pkl")
            with open(metadata_file, 'rb') as f:
                metadata = pickle.load(f)
            
            self.documents = metadata['documents']
            
            print(f"âœ… å‘é‡æ•°æ®åº“åŠ è½½å®Œæˆï¼ŒåŒ…å« {len(self.documents)} ä¸ªæ–‡æ¡£å—")
            
        except Exception as e:
            logger.error(f"åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            raise
    
    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        æœç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›çš„æ–‡æ¡£æ•°é‡
            
        Returns:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.embedding_model.encode([query])
            query_embedding = query_embedding.astype('float32')
            faiss.normalize_L2(query_embedding)
            
            # æœç´¢ç›¸ä¼¼å‘é‡
            scores, indices = self.vector_index.search(query_embedding, top_k)
            
            # æ„å»ºç»“æœ
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx < len(self.documents):
                    doc = self.documents[idx]
                    results.append({
                        'content': doc.page_content,
                        'metadata': doc.metadata,
                        'score': float(score)
                    })
            
            return results
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            return []
    
    def query(self, question: str, max_context_length: int = 4000) -> Dict[str, Any]:
        """
        æŸ¥è¯¢åŒ»ç–—é—®é¢˜
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            max_context_length: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
            
        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        try:
            print(f"ğŸ” æ­£åœ¨æŸ¥è¯¢: {question}")
            
            # æœç´¢ç›¸å…³æ–‡æ¡£
            search_results = self.search(question, top_k=5)
            
            if not search_results:
                return {
                    'answer': 'æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚å»ºè®®æ‚¨å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿã€‚',
                    'sources': [],
                    'confidence': 0.0
                }
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            sources = []
            total_length = 0
            
            for result in search_results:
                content = result['content']
                metadata = result['metadata']
                score = result['score']
                
                if total_length + len(content) > max_context_length:
                    break
                
                context_parts.append(content)
                sources.append({
                    'source': metadata.get('source', 'unknown'),
                    'index': metadata.get('index', -1),
                    'question': metadata.get('question', ''),
                    'score': score
                })
                total_length += len(content)
            
            context = "\n\n".join(context_parts)
            
            # æ„å»ºæç¤ºè¯
            prompt = f"""
åŸºäºä»¥ä¸‹åŒ»å­¦çŸ¥è¯†åº“ä¿¡æ¯ï¼Œè¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š

ç”¨æˆ·é—®é¢˜: {question}

ç›¸å…³åŒ»å­¦çŸ¥è¯†:
{context}

è¯·æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„å›ç­”ï¼Œå¹¶è¯´æ˜ä¿¡æ¯æ¥æºã€‚
"""
            
            # è°ƒç”¨Qwenæ¨¡å‹
            messages = [{'role': 'user', 'content': prompt}]
            response = self.assistant.run(messages)
            
            # å¤„ç†å“åº”
            answer = self._extract_answer(response)
            
            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_confidence(search_results)
            
            return {
                'answer': answer,
                'sources': sources,
                'confidence': confidence,
                'context_length': total_length
            }
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                'answer': f'æŸ¥è¯¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}',
                'sources': [],
                'confidence': 0.0
            }
    
    def _extract_answer(self, response) -> str:
        """æå–æ¨¡å‹å›ç­”"""
        try:
            if hasattr(response, '__iter__'):
                response_list = list(response)
                if len(response_list) > 0:
                    # æŸ¥æ‰¾æœ€åä¸€ä¸ªæœ‰å†…å®¹çš„assistantæ¶ˆæ¯
                    for msg_list in reversed(response_list):
                        if isinstance(msg_list, list) and len(msg_list) > 0:
                            for msg in msg_list:
                                if isinstance(msg, dict) and msg.get('role') == 'assistant':
                                    content = msg.get('content', '')
                                    if content and content.strip():
                                        return content
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°assistantæ¶ˆæ¯ï¼Œè¿”å›æœ€åä¸€ä¸ªæ¶ˆæ¯çš„å†…å®¹
                    last_msg_list = response_list[-1]
                    if isinstance(last_msg_list, list) and len(last_msg_list) > 0:
                        last_msg = last_msg_list[-1]
                        if isinstance(last_msg, dict):
                            return last_msg.get('content', 'æ— æ³•è·å–å›ç­”')
                        else:
                            return str(last_msg)
            
            return 'æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å¤„ç†æ‚¨çš„é—®é¢˜ã€‚'
            
        except Exception as e:
            logger.error(f"æå–å›ç­”å¤±è´¥: {e}")
            return 'æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å¤„ç†æ‚¨çš„é—®é¢˜ã€‚'
    
    def _calculate_confidence(self, search_results: List[Dict]) -> float:
        """è®¡ç®—å›ç­”ç½®ä¿¡åº¦"""
        if not search_results:
            return 0.0
        
        # åŸºäºæœç´¢ç»“æœçš„ç›¸ä¼¼åº¦åˆ†æ•°è®¡ç®—ç½®ä¿¡åº¦
        scores = [result['score'] for result in search_results]
        avg_score = sum(scores) / len(scores)
        
        # å°†ç›¸ä¼¼åº¦åˆ†æ•°è½¬æ¢ä¸ºç½®ä¿¡åº¦ (0-1)
        confidence = min(avg_score * 2, 1.0)  # ç®€å•çš„çº¿æ€§æ˜ å°„
        
        return confidence
    
    def get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        return {
            'embedding_model': self.embedding_model_name,
            'chunk_size': self.chunk_size,
            'chunk_overlap': self.chunk_overlap,
            'total_documents': len(self.documents),
            'vector_db_path': self.vector_db_path,
            'medical_data_path': self.medical_data_path,
            'vector_index_size': self.vector_index.ntotal if self.vector_index else 0
        }

def main():
    """ä¸»å‡½æ•° - æµ‹è¯•RAGç³»ç»Ÿ"""
    print("ğŸ¥ åŒ»ç–—RAGç³»ç»Ÿæµ‹è¯• - ç‰ˆæœ¬1")
    print("=" * 60)
    
    try:
        # åˆå§‹åŒ–RAGç³»ç»Ÿ
        rag_system = MedicalRAGSystem()
        
        # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
        info = rag_system.get_system_info()
        print(f"\nğŸ“Š ç³»ç»Ÿä¿¡æ¯:")
        for key, value in info.items():
            print(f"   {key}: {value}")
        
        # æµ‹è¯•æŸ¥è¯¢
        test_questions = [
            "ä»€ä¹ˆæ˜¯é«˜è¡€å‹ï¼Ÿ",
            "æ„Ÿå†’çš„ç—‡çŠ¶æœ‰å“ªäº›ï¼Ÿ",
            "å¦‚ä½•é¢„é˜²ç³–å°¿ç—…ï¼Ÿ",
            "å¿ƒè„ç—…çš„æ—©æœŸç—‡çŠ¶æ˜¯ä»€ä¹ˆï¼Ÿ"
        ]
        
        for question in test_questions:
            print(f"\nğŸ” æµ‹è¯•é—®é¢˜: {question}")
            result = rag_system.query(question)
            
            print(f"ğŸ“ å›ç­”: {result['answer'][:200]}...")
            print(f"ğŸ¯ ç½®ä¿¡åº¦: {result['confidence']:.2f}")
            print(f"ğŸ“š æ¥æºæ•°é‡: {len(result['sources'])}")
            print("-" * 40)
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
